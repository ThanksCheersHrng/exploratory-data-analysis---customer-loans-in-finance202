{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page is taking over the functionality of ongoing_workspace.ipynb, so that I can import the data frame as it is currently saved, and run things more efficiently, now that Tasks 1-4 are complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble - imports \n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import data_cleaning_for_EDA as dc \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imputing_methods as im "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble - Multiple Inheritance Class \n",
    "class MultipleInheritanceTestClass(im.Plotter, im.DataFrameTransform, dc.DataFrameInfo):\n",
    "    def export_to_csv(self, filename):\n",
    "        self.data_frame.to_csv(filename, index=False)\n",
    "        print(f\"DataFrame exported to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preamble - data read in\n",
    "df_df = pd.read_csv(\"df_end_of_Task_4.csv\")\n",
    "# ensure it's a data frame at this stage so everything inherits correctly in the next step. \n",
    "type(df_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MultipleInheritanceTestClass(df_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Remove outliers  (Let the record show that as a statistician I do not condone willfully ignoring extereme-yet-possible data! I do so for the purposes of this task.)\n",
    "\n",
    "Step 1: Visualise with Plotter class to determine if coumns contain outliers. \n",
    "I think the best way to indicate that something is an outlier is to plot the actual values against the expected values (were the data Gaussian) in a QQNorm plot. However, since I already plotted loads of this data and identified many non-Gaussian-looking distributions (that's even after \"skew adjustment,\" another statistically questionable practise), I don't think a Normal QQ plot would tell me anything meaningful about outliers. \n",
    "\n",
    "I think the best I can offer right now is a box plot. It turs out there's a handy showfliers argument in the sns.boxplot() method-- see Plotter class, where I've added the boxplot_with_outliers() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting checks \n",
    "print(type(df))\n",
    "\n",
    "print(type(df.col_names()))\n",
    "\n",
    "df.stats()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.col_names()\n",
    "\n",
    "for column in column_names:\n",
    "        print(f\"Plotting boxplot for {column}:\")\n",
    "        df.boxplot_with_outliers(column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So many of the distributions of these columns struck me as Non-Gaussian to begin with (e.g. exponential or poisson could fit a lot of this data, since we are literally counting \"how many accounts have this quality?\" and the number of accounts accrues as time passes.), so the concept of using 1.5*IQR as the defining \"distance from the quartile\" for outliers (which is what sbn.boxplot() does) may not be appropriate. If I see a boxplot has 50 \"outliers,\" they're not really outliers, are they? However, these boxplots give me an initial visual so if I see anything that's truly separate from the pack, I can consider removing it. \n",
    "\n",
    "I also won't be removing anything from the id or member_id columns, since these aren't *really* quantitative data. \n",
    "\n",
    "So, items to consider removing, depending on how many are there: \n",
    "\n",
    "\n",
    "*collection_recovery_fee - has roughly four values higher than 5.5K, but honestly the way the plot tapers suggests exponential/poisson distribution so I wouldn't cut these off willy nilly. \n",
    "\n",
    "*recoveries - similar to the above, for values above 17500 \n",
    "\n",
    "*total_rec_late_fee - because the density is so much more powerful and drops off so suddenly after 150, I'd be more inclined to remove the four or so that are greater than 150 in this column. \n",
    "\n",
    "*total_rec_int - there's one that's probably a couple hundred higher than the rest, but the tapering of frequencies shown in this plot suggests another poisson. \n",
    "\n",
    "total_accounts - I could consider removing the 5 that are about 85-ish, or just the 2 that are above 90. There's still evidence of tapering which calls the legitimacy of dropping outliers into question. \n",
    "\n",
    "open_accounts - clearly discrete dta, with 5 above 45 and 2 above 50, and the thickness of the circles indicates tapering, i.e. a poisson-style distribution. As the values are all fairly close to each other, I wouldn't be inclined to remove any of those. \n",
    "\n",
    "*inq_last_6mths - the four above 22 are quite 'gapped' away from the rest, especially the three above 30. \n",
    "\n",
    "*deling_2yrs - looks like tapering to me. we'll see how spikey the values above 15 look in a histogram. \n",
    "\n",
    "Next I'll examine histograms for the columns with stars, above, to deterimine if in fact the \"outliers\" the box plot highlighted were isolated and far from the trend, or simply the natural tails of a tapering distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "spiky_columns = list({\"collection_recovery_fee\", \"recoveries\", \"total_rec_late_fee\", \"total_rec_int\", \"inq_last_6mths\",\"delinq_2yrs\"}) \n",
    "print(type(spiky_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After much nashing of teeth and growling in an attempt to turn each histogram's y-axis (which is strictly frequency density), a friend suggested I just look at a table to help me judge how far out and unexpected the one-off's are. Let's see if this works... the table() method is added to the Plotter class... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for delinq_2yrs:\n",
      "    Value  Count\n",
      "0       0  45631\n",
      "1       1   5956\n",
      "2       2   1690\n",
      "3       3    531\n",
      "4       4    215\n",
      "5       5     93\n",
      "6       6     53\n",
      "7       7     26\n",
      "8       8     15\n",
      "9      10      7\n",
      "10      9      4\n",
      "11     11      4\n",
      "12     13      3\n",
      "13     17      1\n",
      "14     15      1\n",
      "15     18      1\n",
      "Table for total_rec_int:\n",
      "         Value  Count\n",
      "0         0.00     73\n",
      "1      1745.58     12\n",
      "2      1593.98     11\n",
      "3      1062.44     11\n",
      "4      1280.57     10\n",
      "...        ...    ...\n",
      "49326  3894.18      1\n",
      "49327  1707.15      1\n",
      "49328  2133.94      1\n",
      "49329  2556.50      1\n",
      "49330   352.34      1\n",
      "\n",
      "[49331 rows x 2 columns]\n",
      "Table for inq_last_6mths:\n",
      "    Value  Count\n",
      "0       0  27110\n",
      "1       1  14969\n",
      "2       2   6917\n",
      "3       3   3296\n",
      "4       4   1035\n",
      "5       5    458\n",
      "6       6    213\n",
      "7       7    101\n",
      "8       8     65\n",
      "9       9     19\n",
      "10     10     11\n",
      "11     11      9\n",
      "12     12      6\n",
      "13     15      6\n",
      "14     13      4\n",
      "15     16      3\n",
      "16     18      2\n",
      "17     17      1\n",
      "18     33      1\n",
      "19     24      1\n",
      "20     31      1\n",
      "21     32      1\n",
      "22     19      1\n",
      "23     14      1\n",
      "Table for collection_recovery_fee:\n",
      "         Value  Count\n",
      "0       0.0000  50788\n",
      "1       1.4400      7\n",
      "2       1.6900      6\n",
      "3       1.7300      6\n",
      "4       2.0800      6\n",
      "...        ...    ...\n",
      "2939  506.8728      1\n",
      "2940   75.6270      1\n",
      "2941    2.7103      1\n",
      "2942    2.1780      1\n",
      "2943    0.2300      1\n",
      "\n",
      "[2944 rows x 2 columns]\n",
      "Table for recoveries:\n",
      "        Value  Count\n",
      "0        0.00  50567\n",
      "1       16.27      3\n",
      "2       19.20      3\n",
      "3        9.80      3\n",
      "4       10.92      2\n",
      "...       ...    ...\n",
      "3594  1089.14      1\n",
      "3595   947.16      1\n",
      "3596  1417.31      1\n",
      "3597   493.03      1\n",
      "3598   182.27      1\n",
      "\n",
      "[3599 rows x 2 columns]\n",
      "Table for total_rec_late_fee:\n",
      "           Value  Count\n",
      "0       0.000000  52470\n",
      "1      15.000000    251\n",
      "2      30.000000     29\n",
      "3      24.910000      4\n",
      "4      32.730000      4\n",
      "...          ...    ...\n",
      "1434   14.948799      1\n",
      "1435   29.390000      1\n",
      "1436   31.630000      1\n",
      "1437   19.962410      1\n",
      "1438  104.942381      1\n",
      "\n",
      "[1439 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "for column in spiky_columns: \n",
    "    print(f\"Table for {column}:\")\n",
    "    df.table(column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA_AiCore_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
