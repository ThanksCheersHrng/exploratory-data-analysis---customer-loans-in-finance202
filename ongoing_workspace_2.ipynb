{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page is taking over the functionality of ongoing_workspace.ipynb, so that I can import the data frame as it is currently saved, and run things more efficiently, now that Tasks 1-4 are complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble - imports \n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import data_cleaning_for_EDA as dc \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imputing_methods as im "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble - Multiple Inheritance Class \n",
    "class MultipleInheritanceTestClass(im.Plotter, im.DataFrameTransform, dc.DataFrameInfo):\n",
    "    def export_to_csv(self, filename):\n",
    "        self.data_frame.to_csv(filename, index=False)\n",
    "        print(f\"DataFrame exported to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble - data read in\n",
    "df_df = pd.read_csv(\"df_end_of_Task_4.csv\")\n",
    "# ensure it's a data frame at this stage so everything inherits correctly in the next step. \n",
    "type(df_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MultipleInheritanceTestClass(df_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Remove outliers  \n",
    "(Let the record show that as a statistician I do not condone willfully ignoring extereme-yet-possible data! I do so for the purposes of this task.)\n",
    "\n",
    "Step 1: Visualise with Plotter class to determine if coumns contain outliers. \n",
    "I think the best way to indicate that something is an outlier is to plot the actual values against the expected values (were the data Gaussian) in a QQNorm plot. However, since I already plotted loads of this data and identified many non-Gaussian-looking distributions (that's even after \"skew adjustment,\" another statistically questionable practise), I don't think a Normal QQ plot would tell me anything meaningful about outliers. \n",
    "\n",
    "I think the best I can offer right now is a box plot. It turs out there's a handy showfliers argument in the sns.boxplot() method-- see Plotter class, where I've added the boxplot_with_outliers() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting checks \n",
    "print(type(df))\n",
    "\n",
    "print(type(df.col_names()))\n",
    "\n",
    "df.stats()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.col_names()\n",
    "\n",
    "for column in column_names:\n",
    "        print(f\"Plotting boxplot for {column}:\")\n",
    "        df.boxplot_with_outliers(column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So many of the distributions of these columns struck me as Non-Gaussian to begin with (e.g. exponential or poisson could fit a lot of this data, since we are literally counting \"how many accounts have this quality?\" and the number of accounts accrues as time passes.), so the concept of using 1.5*IQR as the defining \"distance from the quartile\" for outliers (which is what sbn.boxplot() does) may not be appropriate. If I see a boxplot has 50 \"outliers,\" they're not really outliers, are they? However, these boxplots give me an initial visual so if I see anything that's truly separate from the pack, I can consider removing it. \n",
    "\n",
    "I also won't be removing anything from the id or member_id columns, since these aren't *really* quantitative data. \n",
    "\n",
    "So, items to consider removing, depending on how many are there: \n",
    "\n",
    "\n",
    "*collection_recovery_fee - has roughly four values higher than 5.5K, but honestly the way the plot tapers suggests exponential/poisson distribution so I wouldn't cut these off willy nilly. \n",
    "\n",
    "*recoveries - similar to the above, for values above 17500 \n",
    "\n",
    "*total_rec_late_fee - because the density is so much more powerful and drops off so suddenly after 150, I'd be more inclined to remove the four or so that are greater than 150 in this column. \n",
    "\n",
    "*total_rec_int - there's one that's probably a couple hundred higher than the rest, but the tapering of frequencies shown in this plot suggests another poisson. \n",
    "\n",
    "total_accounts - I could consider removing the 5 that are about 85-ish, or just the 2 that are above 90. There's still evidence of tapering which calls the legitimacy of dropping outliers into question. \n",
    "\n",
    "open_accounts - clearly discrete dta, with 5 above 45 and 2 above 50, and the thickness of the circles indicates tapering, i.e. a poisson-style distribution. As the values are all fairly close to each other, I wouldn't be inclined to remove any of those. \n",
    "\n",
    "*inq_last_6mths - the four above 22 are quite 'gapped' away from the rest, especially the three above 30. \n",
    "\n",
    "*deling_2yrs - looks like tapering to me. we'll see how spikey the values above 15 look in a histogram. \n",
    "\n",
    "Next I'll examine histograms for the columns with stars, above, to deterimine if in fact the \"outliers\" the box plot highlighted were isolated and far from the trend, or simply the natural tails of a tapering distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiky_columns = list({\"collection_recovery_fee\", \"recoveries\", \"total_rec_late_fee\", \"total_rec_int\", \"inq_last_6mths\",\"delinq_2yrs\"}) \n",
    "print(type(spiky_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After much nashing of teeth and growling in an attempt to turn each histogram's y-axis (which is strictly frequency density), a friend suggested I just look at a table to help me judge how far out and unexpected the one-off's are. Let's see if this works... the table() method is added to the Plotter class... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in spiky_columns: \n",
    "    print(f\"Table for {column}:\")\n",
    "    df.table(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to re-examine for a notable gap in some of the tables that were too long to examine from the above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in spiky_columns:\n",
    "        print(f\"Plotting boxplot for {column}:\")\n",
    "        df.boxplot_with_outliers(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, plan of action for outliers has been formed... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of those reviewed above, here are my comments on what outliers to remove: \n",
    "\n",
    "- collection_recovery_fee - looks like they're all counts of 1 (for a total of just a handful) so just cut off anything bigger than 5000\n",
    "- recoveries - looks like they're all counts of 1 so cut off anything bigger than 18000\n",
    "- total_rec_late_fee - cut off anything bigger than 200 \n",
    "- total_rec_int -  looks like they're all 1's but quite close together for a long time, so cut off the top four, i.e. anything bigger than 21500  \n",
    "- inq_last_6mths - I can remove the four above 20. \n",
    "- deling_2yrs - None of those seem far enough away from a poisson-type distribution to justify dropping off. \n",
    "\n",
    "I'll also want to know how many data points I lose as a result of all these cut-offs. I'll be removing roughly 4 from each of the columns above, so if my loss of data points amounts to less than 20, I'll know some of those values were related by virtue of being part of the same data point (which suggests they could have been leverage points, but oh well.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.print_shape()\n",
    "df.remove_outlier_rows('collection_recovery_fee', 5000)\n",
    "df.print_shape() # lost 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.remove_outlier_rows('recoveries', 18000)\n",
    "df.print_shape() # lost 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.print_shape()\n",
    "df.remove_outlier_rows('total_rec_late_fee', 200)\n",
    "df.print_shape() # lost 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.remove_outlier_rows('total_rec_int', 21500)\n",
    "df.print_shape() # lost 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.remove_outlier_rows('inq_last_6mths', 20)\n",
    "df.print_shape() # lost 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's promising to see from the losses above that there wasn't any overlap, so we moved simple outliers rather than leverage points. \n",
    "I'd now like to save this data frame for the next task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.export_to_csv(\"df_end_of_Task_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Dropping overly correlated columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_csv(\"df_end_of_Task_5.csv\")\n",
    "# ensure it's a data frame at this stage so everything inherits correctly in the next step. \n",
    "type(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MultipleInheritanceTestClass(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tasked with examining high-correlation pairs of columns. See the corr_matrix() method in DataFrameInfo(), from data_cleaning_for_EDA module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method will produce a load of pngs to display scatterplots... repo, prepare for a file dump. XD \n",
    "df.corr_matrix() #default is 0.65 as a high correlation threshold; I'm happy with that for a start. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly examine the png files now in this repo if you'd like to see scatterplots of the high-correlation columns to remove. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, if you remove all high correlation columns, then you lose meaningful information. That is, if A and B are both related to C, but A and B are highly correlated with each other, then if you remove both A and B, you lose information on the relationship to C. \n",
    "\n",
    "By looking for the most frequent and eggregious \"offenders\" of colinearity among those plots, I identified the following as possible columns to remove: \n",
    "- funded_amount \n",
    "- out_prncp \n",
    "- id or member_id (they're basically identical; I just want to check which one has a nicer distrubtion, and I'll keep that one.) \n",
    "- total_payment \n",
    "- total_rec_prncp \n",
    "\n",
    "My eyeballs doesn't feel like a rigorous enough decision making tool with so many high correlation pairs to examine (thoguh I'm fairly certain there are duplicates, i.e. A vs B and B vs A are shown on separate plots), so I will scour the internet for a more meaningful selection process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately I worked out a list of highly correlated column pairs, and opted to remove the column with greater skew for each pair until there were no pairs left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I need to save id and member_id despite their being highly correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_these_columns = ('id', 'member_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the goodbye_high_corr_cols() method in the DataFrameTransform, I will\n",
    "- identify pairs of columns with correlations higher than 0.7 \n",
    "    - the pairs can be listed as tuples\n",
    "- iterate over that list: \n",
    "    - calculate the skewness of each column in the pair \n",
    "    - remove the column with the higher skewness \n",
    "- iterate until all pairs of highly correlated columns are gone \n",
    "Implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'id', 'member_id', 'loan_amount', 'funded_amount', 'funded_amount_inv', 'term_length_in_months', 'int_rate', 'instalment', 'grade', 'sub_grade', 'years_of_employment', 'home_ownership', 'annual_inc', 'verification_status', 'issue_date', 'loan_status', 'payment_plan', 'purpose', 'dti', 'delinq_2yrs', 'earliest_credit_line', 'inq_last_6mths', 'open_accounts', 'total_accounts', 'out_prncp', 'out_prncp_inv', 'total_payment', 'total_payment_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_payment_date', 'last_payment_amount', 'next_payment_date', 'last_credit_pull_date', 'collections_12_mths_ex_med', 'policy_code', 'application_type']\n",
      "['Unnamed: 0', 'id', 'member_id', 'loan_amount', 'term_length_in_months', 'int_rate', 'grade', 'sub_grade', 'years_of_employment', 'home_ownership', 'annual_inc', 'verification_status', 'issue_date', 'loan_status', 'payment_plan', 'purpose', 'dti', 'delinq_2yrs', 'earliest_credit_line', 'inq_last_6mths', 'open_accounts', 'total_accounts', 'out_prncp', 'total_rec_prncp', 'total_rec_late_fee', 'recoveries', 'last_payment_date', 'last_payment_amount', 'next_payment_date', 'last_credit_pull_date', 'collections_12_mths_ex_med', 'policy_code', 'application_type']\n"
     ]
    }
   ],
   "source": [
    "print(df.col_names())\n",
    "df.goodbye_high_corr_cols(columns_to_keep=save_these_columns) \n",
    "print(df.col_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54210, 33)\n"
     ]
    }
   ],
   "source": [
    "df.print_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame exported to df_after_Task_6.csv\n"
     ]
    }
   ],
   "source": [
    "df.export_to_csv(\"df_after_Task_6.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 and 8 are about updating code changes to Github (which I do constantly), and Refactoring and optimising code (which I also do while in progresss/development.) \n",
    "Therefore, I think I'm done working from ongoing_workspace_2.ipynb. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA_AiCore_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
