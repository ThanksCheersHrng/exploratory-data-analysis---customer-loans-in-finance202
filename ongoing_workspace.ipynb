{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import data_cleaning_for_EDA as dc \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imputing_methods as im\n",
    "# from imputing_methods import Plotter, DataFrameTransform\n",
    "# from data_cleaning_for_EDA import DataFrameInfo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_datetime_mo_yr(date_str):\n",
    "    return pd.to_datetime(date_str, format='%b-%Y', errors='coerce') \n",
    "\n",
    "# Reading in the csv and parsing dates. \n",
    "finance_df = pd.read_csv(\n",
    "    \"dataframe.csv\",\n",
    "    parse_dates=['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date'],\n",
    "    date_format='%m-%Y',\n",
    "    converters={'issue_date': parse_to_datetime_mo_yr, \n",
    "                'earliest_credit_line': parse_to_datetime_mo_yr,\n",
    "                'last_payment_date': parse_to_datetime_mo_yr,\n",
    "                'next_payment_date': parse_to_datetime_mo_yr,\n",
    "                'last_credit_pull_date': parse_to_datetime_mo_yr}\n",
    ")\n",
    "# Cleaning up lengths of time that could be treated as floats. \n",
    "finance_df['employment_length'] = finance_df['employment_length'].str.extract(r\"([-+]?\\d*\\.\\d+|[-+]?\\d+)\").astype(float)\n",
    "finance_df['term'] = finance_df['term'].str.extract(r\"([-+]?\\d*\\.\\d+|[-+]?\\d+)\").astype(float)\n",
    "finance_df.rename(columns = {'employment_length':'years_of_employment', 'term' : 'term_length_in_months'}, inplace = True) \n",
    "# Cleaning up object type columns- all of which are suitable for category (I checked).\n",
    "columns_to_cat = ['grade', 'sub_grade', 'verification_status', 'home_ownership', 'loan_status', 'payment_plan', 'purpose', 'application_type']\n",
    "finance_df[columns_to_cat] = finance_df[columns_to_cat].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowing my data frame to inherit any classes I've created and want to use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleInheritanceTestClass(im.Plotter, im.DataFrameTransform, dc.DataFrameInfo):\n",
    "    def export_to_csv(self, filename):\n",
    "        self.data_frame.to_csv(filename, index=False)\n",
    "        print(f\"DataFrame exported to {filename}\")\n",
    "\n",
    "df = MultipleInheritanceTestClass(finance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Workspace: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                      0.000000\n",
      "id                              0.000000\n",
      "member_id                       0.000000\n",
      "loan_amount                     0.000000\n",
      "funded_amount                   5.544799\n",
      "funded_amount_inv               0.000000\n",
      "term_length_in_months           8.799395\n",
      "int_rate                        9.531449\n",
      "instalment                      0.000000\n",
      "grade                           0.000000\n",
      "sub_grade                       0.000000\n",
      "years_of_employment             3.905515\n",
      "home_ownership                  0.000000\n",
      "annual_inc                      0.000000\n",
      "verification_status             0.000000\n",
      "issue_date                      0.000000\n",
      "loan_status                     0.000000\n",
      "payment_plan                    0.000000\n",
      "purpose                         0.000000\n",
      "dti                             0.000000\n",
      "delinq_2yrs                     0.000000\n",
      "earliest_credit_line            0.000000\n",
      "inq_last_6mths                  0.000000\n",
      "mths_since_last_delinq         57.166565\n",
      "mths_since_last_record         88.602460\n",
      "open_accounts                   0.000000\n",
      "total_accounts                  0.000000\n",
      "out_prncp                       0.000000\n",
      "out_prncp_inv                   0.000000\n",
      "total_payment                   0.000000\n",
      "total_payment_inv               0.000000\n",
      "total_rec_prncp                 0.000000\n",
      "total_rec_int                   0.000000\n",
      "total_rec_late_fee              0.000000\n",
      "recoveries                      0.000000\n",
      "collection_recovery_fee         0.000000\n",
      "last_payment_date               0.000000\n",
      "last_payment_amount             0.000000\n",
      "next_payment_date               0.000000\n",
      "last_credit_pull_date           0.000000\n",
      "collections_12_mths_ex_med      0.094042\n",
      "mths_since_last_major_derog    86.172116\n",
      "policy_code                     0.000000\n",
      "application_type                0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Before: \n",
    "df.perc_null()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                    0.000000\n",
      "id                            0.000000\n",
      "member_id                     0.000000\n",
      "loan_amount                   0.000000\n",
      "funded_amount                 5.544799\n",
      "funded_amount_inv             0.000000\n",
      "term_length_in_months         8.799395\n",
      "int_rate                      9.531449\n",
      "instalment                    0.000000\n",
      "grade                         0.000000\n",
      "sub_grade                     0.000000\n",
      "years_of_employment           3.905515\n",
      "home_ownership                0.000000\n",
      "annual_inc                    0.000000\n",
      "verification_status           0.000000\n",
      "issue_date                    0.000000\n",
      "loan_status                   0.000000\n",
      "payment_plan                  0.000000\n",
      "purpose                       0.000000\n",
      "dti                           0.000000\n",
      "delinq_2yrs                   0.000000\n",
      "earliest_credit_line          0.000000\n",
      "inq_last_6mths                0.000000\n",
      "open_accounts                 0.000000\n",
      "total_accounts                0.000000\n",
      "out_prncp                     0.000000\n",
      "out_prncp_inv                 0.000000\n",
      "total_payment                 0.000000\n",
      "total_payment_inv             0.000000\n",
      "total_rec_prncp               0.000000\n",
      "total_rec_int                 0.000000\n",
      "total_rec_late_fee            0.000000\n",
      "recoveries                    0.000000\n",
      "collection_recovery_fee       0.000000\n",
      "last_payment_date             0.000000\n",
      "last_payment_amount           0.000000\n",
      "next_payment_date             0.000000\n",
      "last_credit_pull_date         0.000000\n",
      "collections_12_mths_ex_med    0.094042\n",
      "policy_code                   0.000000\n",
      "application_type              0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.drop_weak_columns() \n",
    "df.perc_null()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide whether the column should be imputed with median or mean: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I ran plots for all five of the non-zero percentage null columns, but I kept pressing shift+enter so moved on with my life; you can see three here and run the other three yourself if you want. \n",
    "df.plot_column(\"int_rate\")\n",
    "df.plot_column(\"years_of_employment\")\n",
    "df.plot_column(\"collections_12_mths_ex_med\")\n",
    "#they're all skewed so all median imputes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                    0.0\n",
      "id                            0.0\n",
      "member_id                     0.0\n",
      "loan_amount                   0.0\n",
      "funded_amount                 0.0\n",
      "funded_amount_inv             0.0\n",
      "term_length_in_months         0.0\n",
      "int_rate                      0.0\n",
      "instalment                    0.0\n",
      "grade                         0.0\n",
      "sub_grade                     0.0\n",
      "years_of_employment           0.0\n",
      "home_ownership                0.0\n",
      "annual_inc                    0.0\n",
      "verification_status           0.0\n",
      "issue_date                    0.0\n",
      "loan_status                   0.0\n",
      "payment_plan                  0.0\n",
      "purpose                       0.0\n",
      "dti                           0.0\n",
      "delinq_2yrs                   0.0\n",
      "earliest_credit_line          0.0\n",
      "inq_last_6mths                0.0\n",
      "open_accounts                 0.0\n",
      "total_accounts                0.0\n",
      "out_prncp                     0.0\n",
      "out_prncp_inv                 0.0\n",
      "total_payment                 0.0\n",
      "total_payment_inv             0.0\n",
      "total_rec_prncp               0.0\n",
      "total_rec_int                 0.0\n",
      "total_rec_late_fee            0.0\n",
      "recoveries                    0.0\n",
      "collection_recovery_fee       0.0\n",
      "last_payment_date             0.0\n",
      "last_payment_amount           0.0\n",
      "next_payment_date             0.0\n",
      "last_credit_pull_date         0.0\n",
      "collections_12_mths_ex_med    0.0\n",
      "policy_code                   0.0\n",
      "application_type              0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.impute_null_values()\n",
    "df.perc_null() # All 0s now, as should be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise the removal of null values with another plot: \n",
    "df.plot_column(\"int_rate\")\n",
    "df.plot_column(\"years_of_employment\")\n",
    "df.plot_column(\"collections_12_mths_ex_med\")\n",
    "#As expected, now major spikes at point of median-- this would increase kurtosis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Dealing with Skew. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preamble (speedrun through Task 3 on startup)\n",
    "# 1. Run imports \n",
    "# 2. Read-in finance_df \n",
    "\n",
    "class MultipleInheritanceTestClass(im.Plotter, im.DataFrameTransform, dc.DataFrameInfo):\n",
    "        def export_to_csv(self, file_name):\n",
    "            self.data_frame.to_csv(file_name, index=False)\n",
    "            print(f\"DataFrame exported to {file_name}\")\n",
    "\n",
    "df = MultipleInheritanceTestClass(finance_df)\n",
    "df.drop_weak_columns() \n",
    "df.impute_null_values() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Identify skewed columns in data using Pandas methods; determine a threshold for skewness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a high_skew_columns method in data_clening_for_EDA. It should give any columns with skew greater than 1.2 (default threshold). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finance_df.skew(numeric_only=True)\n",
    "df.high_skew_columns() # With a threshold of 1.2, this yielded: \n",
    "'''\n",
    "['id',\n",
    " 'member_id',\n",
    " 'annual_inc',\n",
    " 'delinq_2yrs',\n",
    " 'inq_last_6mths',\n",
    " 'out_prncp',\n",
    " 'out_prncp_inv',\n",
    " 'total_payment',\n",
    " 'total_payment_inv',\n",
    " 'total_rec_prncp',\n",
    " 'total_rec_int',\n",
    " 'total_rec_late_fee',\n",
    " 'recoveries',\n",
    " 'collection_recovery_fee',\n",
    " 'last_payment_amount',\n",
    " 'collections_12_mths_ex_med'] \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Rule of thumb off the internet is a skew of abs val 1 is \"uncomfortable.\" Let's see what those look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot_column(\"term_length_in_months\") #skew of 1 and this is the \"binary\" one I imputed with the median so it now has a ratio of about 8:3... pretty heavily skewed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot_column(\"total_accounts\") # skewness of about 0.8 apparently. Looks like a Normal Distribution with just some extra tail on the top end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot_column(\"total_rec_late_fee\") #skewness of 13 - graph basically looks like it has just one outcome -- very skewed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Once the skewed columns are identified, perform transformations no these columns to determine which transformation results in the biggest reduction in skew. \n",
    "Create a method to transform columns in DataFrameTransform class. \n",
    "\n",
    "As advised by Mat from AiCore, and detailed further here (https://anatomisebiostats.com/biostatistics-blog/transforming-skewed-data/), I've identified the following may be appropriate transformations: \n",
    "\n",
    "- log: potentially appropriate for high, positive skew. the larger the base (e.g. e vs 10), the higher the original degree of skewness. \n",
    "    - reciprocal: Mat didn't mention this, but it may be appropriate for extreme positive skewness. \n",
    "- yeo-johnson: Potentially appropriate for positively skewed data. \n",
    "- box-cox: For positively skewed, exclusively positive data. \n",
    "- square root: potentially appropriate for moderate, positive skew. \n",
    "- cube root: potentially appropriate for negatively skewed data. \n",
    "    - square: potential for negative skew. (again, this was suggested not by Mat but by https://medium.com/@TheDataGyan/day-8-data-transformation-skewness-normalization-and-much-more-4c144d370e55.)\n",
    "\n",
    "I need to be able visualise these transformations one at a time before I commit them to transforming any given column. Therefore, I'll:\n",
    "1. Add a transformation attribute to the plot_column() method. \n",
    "2. Add a printout of the skew values to the high_skew_columns() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all the high-skew plots. \n",
    "high_skew_list = df.high_skew_columns()\n",
    "for column_instance in high_skew_list: \n",
    "    print(f\"Plot of '{column_instance}' with identity transformation:\")\n",
    "    df.plot_column(column_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above we can visually confirm that all the highly skewed columns are right skewed, with skewnesses ranging from moderate (1.26) to crazy extreme (27.64). On the basis of the degree of skewness I will try a few of the following transformations, visualising in plotter before commmitting them to data transformation. Since all major skews were positive, I can rule out the transformations better suited to negative skews. All the key data is non-negative but I can't rule out values of zero. I can select the transformation on the basis of strength of original skewness: \n",
    "- moderate positive skew: square root\n",
    "- high positive skew: natural log \n",
    "- higher positive skew: log base 10 \n",
    "- even higher positive skew: log base 100\n",
    "- extreme positive skew: reciprocal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_transform = lambda x: pd.Series(x).apply(lambda val: val if val < 0 else np.sqrt(val)) #negatives shouldn't be an issue, but just in case... \n",
    "ln_transform = lambda x: pd.Series(x).apply(lambda val: val if val <= 0 else np.log(val)) # log 0 doesn't work either. \n",
    "log10_transform = lambda x: pd.Series(x).apply(lambda val: val if val <= 0 else np.log10(val))\n",
    "log100_transform = lambda x: pd.Series(x).apply(lambda val: val if val <= 0 else np.log(val)/np.log(100)) #numpy doesn't default to log base 100, so need to use log rules. \n",
    "recip_transform = lambda x: pd.Series(x).apply(lambda val: val if val == 0 else np.reciprocal(val)) # reciprocal always exists, except at 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll apply those tranformations and evaluate their prospects according to the list in finance_df.skew().txt (soon to appear in this repo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the skewiest: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot_column(column_name=\"collection_recovery_fee\", transformer = recip_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reciprocal made almost no dent on collection_recovery_fee! Let's try log100: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot_column(column_name=\"collection_recovery_fee\", transformer = log100_transform) # that looks better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up I'll try the reciprocal and log100 on total_rec_late_fee (skew =13), recoveries (skew = 14), and collections_12_mths_ex_med (skew 20). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_3_skews = {\"total_rec_late_fee\", \"recoveries\", \"collections_12_mths_ex_med\"}\n",
    "for column_instance in high_3_skews:\n",
    "    print(f\"Plot of '{column_instance}' with reciprocal transformation:\")\n",
    "    df.plot_column(column_instance, transformer= recip_transform)\n",
    "    print(f\"Plot of '{column_instance}' with log100 transformation:\")\n",
    "    df.plot_column(column_instance, transformer= log100_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selections have been made for the above, see .txt file. \n",
    "Now onto the high, but not as high skews: annual_inc (skew = 8.7), deling_2yrs (5.4), inq_last_6mths (3.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_high_skews = {\"annual_inc\", \"delinq_2yrs\", \"inq_last_6mths\"}\n",
    "for column_instance in mid_high_skews:\n",
    "    print(f\"Plot of '{column_instance}' with log10 transformation:\")\n",
    "    df.plot_column(column_instance, transformer= log10_transform)\n",
    "    print(f\"Plot of '{column_instance}' with log100 transformation:\")\n",
    "    df.plot_column(column_instance, transformer= log100_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was after the above plots that I noticed the log transformations are identical barring their scale, which should not effect skewness. Thus, whether I apply log10 or log100 (or ln) should make no difference to the skewness of the transformed data. An alternative transformation I could have tried was the yeo-johnson transformation but I'm running low on time so I skipped it for now, but I recognise I could have used it (from https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9 : \n",
    "\n",
    "scipy.stats import yeojohnson\n",
    "yf_target, lam = yeojohnson(df[\"TARGET\"]) \n",
    "\n",
    "). \n",
    "\n",
    "Moving on... \n",
    "Then there were loads in the 2-3 range: last_payment_amount, total_rec_int, out_prncp_inv, out_prncp, member_id, id (these last two aren't relevant, as they are ids, not measures of anything in particular). Bear in mind that the dual displays for log10 and ln below should be identical for each column, if my understanding is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_skews = {\"last_payment_amount\", \"total_rec_int\", \"out_prncp_inv\", \"out_prncp\"}\n",
    "for column_instance in mid_skews:\n",
    "    print(f\"Plot of '{column_instance}' with log10 transformation:\")\n",
    "    df.plot_column(column_instance, transformer= log10_transform)\n",
    "    print(f\"Plot of '{column_instance}' with ln transformation:\")\n",
    "    df.plot_column(column_instance, transformer= ln_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Though the graphs look identical across relative scales, the internet assures me that skew is affected differently by different sizes of log base. \n",
    "\n",
    "Finally there were the skews greater than 1.2 but less than 2: total_payment, total_payment_inv, total_rec_prncp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate_skews = {\"total_payment\",\"total_payment_inv\", \"total_rec_prncp\"}\n",
    "for column_instance in moderate_skews:\n",
    "    print(f\"Plot of '{column_instance}' with sqrt transformation:\")\n",
    "    df.plot_column(column_instance, transformer= sqrt_transform)\n",
    "    print(f\"Plot of '{column_instance}' with ln transformation:\")\n",
    "    df.plot_column(column_instance, transformer= ln_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try transforming all those columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation applied to column 'annual_inc'.\n",
      "Transformation applied to column 'delinq_2yrs'.\n",
      "Transformation applied to column 'inq_last_6mths'.\n",
      "Transformation applied to column 'out_prncp'.\n",
      "Transformation applied to column 'out_prncp_inv'.\n",
      "Transformation applied to column 'total_payment'.\n",
      "Transformation applied to column 'total_payment_inv'.\n",
      "Transformation applied to column 'total_rec_prncp'.\n",
      "Transformation applied to column 'total_rec_int'.\n",
      "Transformation applied to column 'total_rec_late_fee'.\n",
      "Transformation applied to column 'recoveries'.\n",
      "Transformation applied to column 'collection_recovery_fee'.\n",
      "Transformation applied to column 'last_payment_amount'.\n",
      "Transformation applied to column 'collections_12_mths_ex_med'.\n"
     ]
    }
   ],
   "source": [
    "#the following selected from finance_df.skew().txt: \n",
    "columns_to_fix = df.high_skew_columns()[2:] # drop the id numbers \n",
    "# print(columns_to_fix) # confirmed the above did what i wanted \n",
    "transforms_to_apply = [log10_transform, log10_transform, log10_transform, log10_transform, log10_transform, sqrt_transform, sqrt_transform, sqrt_transform, ln_transform, log100_transform, log100_transform, log100_transform, log10_transform, recip_transform]\n",
    "\n",
    "# I have confirmed that both of the above are lists. \n",
    "\n",
    "# print(len(columns_to_fix), len(transforms_to_apply)) # of the same length: 14 and 14\n",
    "\n",
    "df.skew_transform(columns_to_fix, transforms_to_apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will save a copy of the df dataframe so I can work with it further without running all that ^ code next time... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame exported to df_end_of_Task_4\n"
     ]
    }
   ],
   "source": [
    "# print(type(df)) # confirms I can't use a pd method directly \n",
    "# so I added an export method to the MultipleInheritanceTestClass() \n",
    "\n",
    "df.export_to_csv(\"df_end_of_Task_4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA_AiCore_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
